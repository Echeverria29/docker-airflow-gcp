# Apache Airflow DAG: Captura de Datos hacia Cloud Storage y BigQuery (con Docker Compose)

Este proyecto muestra c√≥mo construir un pipeline ETL simple pero completo utilizando **Apache Airflow** desplegado localmente con **Docker Compose**.

### ‚úÖ ¬øQu√© hace este DAG?

- üì• **Captura datos de usuarios aleatorios** desde una API p√∫blica (`randomuser.me`).
- üßπ **Elimina duplicados** por correo electr√≥nico antes de cargarlos.
- üóÉÔ∏è **Guarda los datos en archivos CSV √∫nicos** (con timestamp) en el sistema de archivos local.
- ‚òÅÔ∏è **Carga autom√°ticamente estos CSV a Google Cloud Storage**, manteniendo un historial de ejecuciones.
- üìä **Inserta los datos en una tabla de BigQuery**.
- üîÅ **Realiza una deduplicaci√≥n en BigQuery** para mantener solo el √∫ltimo registro por correo electr√≥nico.

### üöÄ Tecnolog√≠as utilizadas

- Apache Airflow (v2+)
- Docker Compose
- Google Cloud Storage
- BigQuery
- Python, Pandas
- API p√∫blica: https://randomuser.me/api/

### üß† Ideal para:

- Practicar flujos ETL reales en entornos locales.
- Entender la integraci√≥n entre Airflow y servicios de Google Cloud.
- Desarrollar y testear pipelines de forma segura antes de mover a producci√≥n.

---

## Requisitos previos

### Herramientas necesarias:

- **WSL2** (si est√°s usando Windows)  
  [Ver video](https://www.youtube.com/watch?v=nkwvDatrKGM&ab_channel=JashTechTV)

- **Docker Desktop**  
  [Ver video](https://www.youtube.com/watch?v=jiJFDwmWrWk&ab_channel=UskoKruM2010)

- **Visual Studio Code + Python** o cualquier editor de c√≥digo  
  [Ver video](https://www.youtube.com/watch?v=1E44n9NL2gw&ab_channel=OssabaTech)

- **Git** (para clonar el repositorio)  
  [Ver video](https://www.youtube.com/watch?v=wVKyeLs0hfg&ab_channel=FerDaniele)

---

## Pasos para levantar el entorno

### Paso 1: Clonar el repositorio

```bash
git clone https://github.com/Echeverria29/docker-airflow-gcp.git
cd docker-airflow-gcp
```
### Puedes desactivar los Dags de ejemplo
AIRFLOW__CORE__LOAD_EXAMPLES: 'False'

### Paso 2: Crear archivo `.env`

Este archivo define el UID de tu usuario para que los contenedores puedan trabajar con permisos correctos.

**En Linux/WSL:**

```bash
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

**En otros sistemas (como Windows):**

```env
AIRFLOW_UID=50000
```

### Paso 3: Inicializar la base de datos

Antes de ejecutar los servicios, es necesario aplicar las migraciones de la base de datos de Airflow y crear el usuario administrador:

```bash
docker compose up airflow-init
```
### Dejar este parametro en False(opcional)
Variable: AIRFLOW__CORE__LOAD_EXAMPLES

(Archivo airflow.cfg)
load_examples = False (opcional)

### Paso 4: Ejecutar Airflow

Ahora puedes iniciar todos los servicios necesarios:

```bash
docker compose up
```
### Antes de ejecutar esperar que todos los conetenedores esten en (healthy)
STATUS Up 2 minutes (healthy)

## Acceso por defecto

- **Usuario:** `airflow`  
- **Contrase√±a:** `airflow`

Airflow estar√° disponible en: [http://localhost:8080](http://localhost:8080)

---
## Conexi√≥n

Paso 1: Abre la interfaz de Airflow
Abre tu navegador y visita:
üëâ http://localhost:8080

üìç Paso 2: Agregar una nueva conexi√≥n
En el men√∫ superior, ve a Admin ‚Üí Connections.

Haz clic en el bot√≥n azul "+ Add a new record" (parte superior derecha).

üìç Paso 3: Completar los campos de la conexi√≥n
Rellena el formulario con esta informaci√≥n:

Campo	Valor
Conn Id	google_cloud_default
Conn Type	Google Cloud
Keyfile JSON	Pega aqu√≠ el contenido completo del archivo JSON de tu service account

üí° Nota: No subas el archivo .json, simplemente abre el archivo con un editor de texto, copia todo su contenido y p√©galo en el campo "Keyfile JSON".

üìç Paso 4: Guardar la conexi√≥n
Haz clic en Save.

Si todo est√° correcto, Airflow ya puede interactuar con BigQuery y Cloud Storage usando esa conexi√≥n.


## Soluci√≥n a errores de permisos en WSL/Linux

En algunos casos, los archivos o carpetas pueden haber sido creados por el usuario `root`, lo cual impide que puedas editarlos. Aqu√≠ te mostramos c√≥mo corregirlo.

### ‚úÖ Cambiar el propietario del archivo o carpeta

Para cambiar un solo archivo:

```bash
sudo chown tu-usuario:tu-usuario /home/tu-usuario/airflow-docker-compose/dags/basic_dag.py
```

Para cambiar todo el directorio del proyecto:

```bash
sudo chown -R tu-usuario:tu-usuario /home/tu-usuario/airflow-docker-compose
```

### ‚úÖ Asegurar permisos de escritura

Para toda la carpeta:

```bash
chmod -R u+rw /home/tu-usuario/airflow-docker-compose
```
---

## Limpieza del entorno

Si necesitas eliminar todo y comenzar desde cero:

```bash
docker compose down --volumes --remove-orphans
rm -rf <directorio-del-proyecto>
```

---

